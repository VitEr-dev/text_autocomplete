{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках учебного проекта сделаны три отдельных решения: первое, для автодополнения слова (LSTM); второе, для автодополнения текста (LSTM); третье, для автодополнения текста (distilgpt2).\n",
    "\n",
    "Каждое решение было сделано в отдельном .ipynb, в отдельных проектных директориях, с учетом локальных настроек. То, как сделано, можно посмотреть здесь: https://github.com/VitEr-dev/text_autocomplete.git. Описание в README.md. Репозиторий публичный.\n",
    "\n",
    "В данном файле отдельные .ipynb просто скопированы из исходных трех, последовательно друг за другом, без адаптации и донастроек. Если запустить выполнение здесь - будут ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ноутбук (solution_LSTM_word.ipynb). Автодополнение слова, архитектура LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Импорт необходимых библиотек\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Добавляем путь к src\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# Импорт наших модулей\n",
    "#import sys\n",
    "#sys.path.append('./src')\n",
    "\n",
    "from data_preprocessing import download_and_load_data, preprocess_data, create_sequence_samples\n",
    "from model import EnhancedLSTMModel\n",
    "from training import train_with_gradient_clipping, custom_collate_fn\n",
    "from evaluation import evaluate_model_on_test, test_model_predictions, save_test_results\n",
    "\n",
    "# Загрузка и предобработка данных\n",
    "print(\"Загрузка данных...\")\n",
    "url = \"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\"\n",
    "tweets_df = download_and_load_data(url)\n",
    "\n",
    "print(len(tweets_df['text']))\n",
    "\n",
    "print(f\"Сохранено {len(tweets_df['text'])} твитов в raw_dataset.csv\")\n",
    "\n",
    "print(\"Предобработка данных...\")\n",
    "clean_df, tokenizer = preprocess_data(tweets_df, save_path='./data/')\n",
    "\n",
    "print(f\"После очистки: {len(clean_df)} твитов\")\n",
    "\n",
    "\n",
    "# Создание последовательностей\n",
    "print(\"Создание обучающих последовательностей...\")\n",
    "def convert_to_list(input_str):\n",
    "    if isinstance(input_str, str):\n",
    "        return [int(x) for x in input_str.strip('[]').split(',') if x.strip()]\n",
    "    return input_str\n",
    "\n",
    "clean_df['input_ids'] = clean_df['input_ids'].apply(convert_to_list)\n",
    "\n",
    "sequence_length = 23\n",
    "X_data, y_data = create_sequence_samples(clean_df['input_ids'].tolist(), sequence_length)\n",
    "\n",
    "print(f\"Создано {len(X_data)} примеров\")\n",
    "print(f\"Форма X: {X_data.shape}, Форма y: {y_data.shape}\")\n",
    "print(f\"Пример X: {X_data[0][:5]}... → y: {y_data[0]}\")\n",
    "\n",
    "# Разделение данных\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Размеры выборок:\")\n",
    "print(f\"Train: {len(X_train)} примеров\")\n",
    "print(f\"Validation: {len(X_val)} примеров\")\n",
    "print(f\"Test: {len(X_test)} примеров\")\n",
    "\n",
    "# Сохранение разделенных данных\n",
    "train_df = pd.DataFrame({'X': X_train.tolist(), 'y': y_train.tolist()})\n",
    "train_df.to_csv('./data/train_dataset.csv', index=False)\n",
    "\n",
    "val_df = pd.DataFrame({'X': X_val.tolist(), 'y': y_val.tolist()})\n",
    "val_df.to_csv('./data/validation_dataset.csv', index=False)\n",
    "\n",
    "test_df = pd.DataFrame({'X': X_test.tolist(), 'y': y_test.tolist()})\n",
    "test_df.to_csv('./data/test_dataset.csv', index=False)\n",
    "\n",
    "# Создание DataLoader'ов\n",
    "batch_size = 256\n",
    "pad_token_id = 0\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: custom_collate_fn(batch, pad_token_id)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: custom_collate_fn(batch, pad_token_id)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: custom_collate_fn(batch, pad_token_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация модели\n",
    "print(\"Инициализация модели...\")\n",
    "model = EnhancedLSTMModel(\n",
    "    vocab_size=30522,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    pad_token_id=0,\n",
    "    dropout=0.5,\n",
    "    use_layer_norm=True,\n",
    "    use_mean_pooling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "print(\"Начало обучения...\")\n",
    "model, train_losses, val_losses, learning_rates = train_with_gradient_clipping(\n",
    "    model, train_loader, val_loader,\n",
    "    num_epochs=3,\n",
    "    learning_rate=0.001,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=1e-5,\n",
    "    model_save_path='./models/trained_model_weights_ELSTM_loc1.pth'\n",
    ")\n",
    "\n",
    "# Сохранение финальной модели\n",
    "torch.save(model.state_dict(), './models/final_model.pth')\n",
    "\n",
    "# Визуализация результатов обучения\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss during Training')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(learning_rates)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/training_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import EnhancedLSTMModel\n",
    "\n",
    "# Тестирование модели\n",
    "print(\"Тестирование модели...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Загрузка лучшей модели для тестирования\n",
    "best_model = EnhancedLSTMModel(\n",
    "    vocab_size=30522,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    pad_token_id=0,\n",
    "    dropout=0.5,\n",
    "    use_layer_norm=True,\n",
    "    use_mean_pooling=False\n",
    ")\n",
    "best_model.load_state_dict(torch.load('./models/final_model.pth'))\n",
    "best_model.to(device)\n",
    "\n",
    "# Оценка на тестовых данных\n",
    "metrics, predictions, targets = evaluate_model_on_test(best_model, test_loader, device)\n",
    "\n",
    "print(\"\\nРЕЗУЛЬТАТЫ ТЕСТИРОВАНИЯ:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Тестирование конкретных предсказаний\n",
    "test_model_predictions(best_model, test_dataset, tokenizer, num_examples=20, device=device)\n",
    "\n",
    "# Сохранение результатов\n",
    "save_test_results(metrics, predictions, targets, './results/test_results.csv')\n",
    "\n",
    "print(\"Все этапы завершены успешно!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ноутбук (solution_LSTM_sequence.ipynb). Автодополнение текста, архитектура LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "from data_preprocessing_sequence import download_and_load_data, preprocess_data, create_text_sequences\n",
    "from model_sequence import EnhancedLSTMModel\n",
    "from training_sequence import train_sequence_model, sequence_collate_fn\n",
    "from evaluation_sequence import evaluate_sequence_model, test_sequence_predictions\n",
    "from rouge_score import rouge_scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка и предобработка данных\n",
    "print(\"Загрузка данных...\")\n",
    "url = \"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\"\n",
    "tweets_df = download_and_load_data(url)\n",
    "\n",
    "print(\"Предобработка данных...\")\n",
    "clean_df, tokenizer = preprocess_data(tweets_df, save_path='./data/')\n",
    "\n",
    "print(\"Создание последовательностей для автодополнения...\")\n",
    "clean_df['input_ids'] = clean_df['input_ids'].apply(\n",
    "    lambda x: [int(i) for i in x.strip('[]').split(',')] if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Создаем последовательности: 75% как вход, 25% как цель\n",
    "X_data, y_data = create_text_sequences(clean_df['input_ids'].tolist(), train_ratio=0.75)\n",
    "\n",
    "print(f\"Создано {len(X_data)} примеров\")\n",
    "print(f\"Длина входа: {len(X_data[0])}, длина цели: {len(y_data[0])}\")\n",
    "\n",
    "# Разделение данных\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Размеры выборок:\")\n",
    "print(f\"Train: {len(X_train)} примеров\")\n",
    "print(f\"Validation: {len(X_val)} примеров\")\n",
    "print(f\"Test: {len(X_test)} примеров\")\n",
    "\n",
    "# Сохранение разделенных данных\n",
    "train_df = pd.DataFrame({'X': X_train, 'y': y_train})\n",
    "train_df.to_csv('./data/train_sequence_dataset.csv', index=False)\n",
    "\n",
    "val_df = pd.DataFrame({'X': X_val, 'y': y_val})\n",
    "val_df.to_csv('./data/validation_sequence_dataset.csv', index=False)\n",
    "\n",
    "test_df = pd.DataFrame({'X': X_test, 'y': y_test})\n",
    "test_df.to_csv('./data/test_sequence_dataset.csv', index=False)\n",
    "\n",
    "# Создание DataLoader'ов\n",
    "batch_size = 32\n",
    "pad_token_id = 0\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    list(zip(X_train, y_train)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: sequence_collate_fn(batch, pad_token_id)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    list(zip(X_val, y_val)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: sequence_collate_fn(batch, pad_token_id)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    list(zip(X_test, y_test)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: sequence_collate_fn(batch, pad_token_id)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация модели\n",
    "print(\"Инициализация модели...\")\n",
    "model = EnhancedLSTMModel(\n",
    "    vocab_size=30522,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    pad_token_id=0,\n",
    "    dropout=0.5,\n",
    "    use_layer_norm=True,\n",
    "    use_mean_pooling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "print(\"Начало обучения...\")\n",
    "model, train_losses, val_losses, train_accuracies, val_accuracies, learning_rates = train_sequence_model(\n",
    "    model, train_loader, val_loader,\n",
    "    num_epochs=3,\n",
    "    learning_rate=0.0005,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=1e-4,\n",
    "    model_save_path='./models/sequence_model.pth'\n",
    ")\n",
    "\n",
    "# Визуализация результатов обучения\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss during Training')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy during Training')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(learning_rates)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/sequence_training_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение финальной модели\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "torch.save(model.state_dict(), './models/final_sequence_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка параметров сохраненной модели\n",
    "checkpoint = torch.load('./models/final_sequence_model.pth', map_location='cpu')\n",
    "print(\"Ключи в state_dict:\")\n",
    "for key in checkpoint.keys():\n",
    "    print(f\"{key}: {checkpoint[key].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование модели\n",
    "print(\"Тестирование модели...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "best_model = EnhancedLSTMModel(\n",
    "    vocab_size=30522,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    pad_token_id=0,\n",
    "    dropout=0.5,\n",
    "    use_layer_norm=True,\n",
    "    use_mean_pooling=False\n",
    ")\n",
    "best_model.load_state_dict(torch.load('./models/final_sequence_model.pth'))\n",
    "best_model.to(device)\n",
    "\n",
    "# Оценка с ROUGE метриками\n",
    "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "metrics, all_predictions, all_targets = evaluate_sequence_model(\n",
    "    best_model, test_loader, tokenizer, rouge_scorer_obj, device\n",
    ")\n",
    "\n",
    "print(\"\\nРЕЗУЛЬТАТЫ ТЕСТИРОВАНИЯ:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Тестирование конкретных предсказаний\n",
    "test_sequence_predictions(best_model, list(zip(X_test, y_test)), tokenizer, \n",
    "                         num_examples=5, device=device)\n",
    "\n",
    "print(\"Все этапы завершены успешно!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ноутбук (solution_distilgpt2.ipynb). Автодополнение текста на предобучененной distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Добавляем путь к src\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# Прямой импорт из модулей\n",
    "from src.model_utils import load_model_and_tokenizer, autocomplete_text\n",
    "from src.data_utils import prepare_test_data, prepare_test_cases\n",
    "from src.evaluation_utils import test_autocomplete_performance\n",
    "from src.evaluation_utils import calculate_rouge_scores\n",
    "from src.evaluation_utils import print_results_analysis \n",
    "from src.evaluation_utils import plot_rouge_scores\n",
    "from src.evaluation_utils import save_results_to_csv\n",
    "\n",
    "\n",
    "# Проверка устройства\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Используемое устройство: {device}\")\n",
    "\n",
    "# КОНФИГУРАЦИЯ ПАРАМЕТРОВ\n",
    "config = {\n",
    "    # Параметры модели\n",
    "    'model_name': \"distilgpt2\",\n",
    "    'max_length': 1024,\n",
    "    'max_new_tokens': 50,\n",
    "    'temperature': 0.7,\n",
    "    'top_k': 50,\n",
    "    'do_sample': True,\n",
    "    \n",
    "    # Параметры данных\n",
    "    'data_path': \"./data/dataset_processed.csv\",\n",
    "    'test_size': 0.5,\n",
    "    'max_samples': 1000,\n",
    "    'split_ratio': 0.7,\n",
    "    'min_length': 20,\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # Параметры оценки\n",
    "    'metrics': ['rouge1', 'rouge2'],\n",
    "    'use_stemmer': True,\n",
    "    \n",
    "    # Параметры сохранения\n",
    "    'save_results': True,\n",
    "    'results_dir': \"./results\",\n",
    "    'results_file': \"autocomplete_results.csv\",\n",
    "    'metrics_file': \"autocomplete_metrics.csv\",\n",
    "    'plot_file': \"rouge_metrics_autocomplete.png\",\n",
    "    \n",
    "    # Параметры тестирования\n",
    "    'num_test_samples': 500\n",
    "}\n",
    "\n",
    "def update_config(**kwargs):\n",
    "    \"\"\"Обновление конфигурации\"\"\"\n",
    "    config.update(kwargs)\n",
    "    print(\"Configuration updated:\")\n",
    "    for key, value in kwargs.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "def run_complete_testing():\n",
    "    \"\"\"Полный pipeline тестирования\"\"\"\n",
    "    print(\"Starting complete autocomplete testing...\")\n",
    "    print(\"Current configuration:\")\n",
    "    for key, value in config.items():\n",
    "        if key not in ['data_path', 'results_dir']:  # Не показываем длинные пути\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # 1. Загрузка модели\n",
    "    model, tokenizer, device = load_model_and_tokenizer(config['model_name'])\n",
    "    \n",
    "    # 2. Подготовка данных\n",
    "    test_texts = prepare_test_data(\n",
    "        csv_path=config['data_path'],\n",
    "        test_size=config['test_size'],\n",
    "        max_samples=config['max_samples'],\n",
    "        random_state=config['random_state']\n",
    "    )\n",
    "    \n",
    "    test_cases = prepare_test_cases(\n",
    "        test_texts,\n",
    "        split_ratio=config['split_ratio'],\n",
    "        min_length=config['min_length']\n",
    "    )\n",
    "    \n",
    "    # 3. Тестирование\n",
    "    results, all_predictions, all_references = test_autocomplete_performance(\n",
    "        model, tokenizer, device, test_cases,\n",
    "        num_samples=config['num_test_samples'],\n",
    "        max_length=config['max_length'],\n",
    "        max_new_tokens=config['max_new_tokens'],\n",
    "        temperature=config['temperature'],\n",
    "        top_k=config['top_k'],\n",
    "        do_sample=config['do_sample']\n",
    "    )\n",
    "    \n",
    "    # 4. Расчет метрик\n",
    "    rouge_scores = calculate_rouge_scores(\n",
    "        all_references, all_predictions,\n",
    "        metrics=config['metrics'],\n",
    "        use_stemmer=config['use_stemmer']\n",
    "    )\n",
    "    \n",
    "    # 5. Анализ результатов\n",
    "    print_results_analysis(results, rouge_scores)\n",
    "    \n",
    "    # 6. Визуализация\n",
    "    plot_rouge_scores(\n",
    "        rouge_scores,\n",
    "        save_plot=config['save_results'],\n",
    "        results_dir=config['results_dir'],\n",
    "        plot_file=config['plot_file']\n",
    "    )\n",
    "    \n",
    "    # 7. Сохранение\n",
    "    save_results_to_csv(\n",
    "        results, rouge_scores,\n",
    "        save_results=config['save_results'],\n",
    "        results_dir=config['results_dir'],\n",
    "        results_file=config['results_file'],\n",
    "        metrics_file=config['metrics_file']\n",
    "    )\n",
    "    \n",
    "    return results, rouge_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск тестирования с конфигурацией по умолчанию\n",
    "print(\"Запуск с конфигурацией по умолчанию:\")\n",
    "results, rouge_scores = run_complete_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\n\" + \"=\"*50)\n",
    "#print(\"ИЗМЕНЕНИЕ КОНФИГУРАЦИИ ДЛЯ СЛЕДУЮЩЕГО ЗАПУСКА\")\n",
    "#print(\"=\"*50)\n",
    "\n",
    "update_config(\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.7,\n",
    "    max_samples=2000,\n",
    "    num_test_samples=1000,\n",
    "    test_size= 0.5\n",
    ")\n",
    "\n",
    "# Запуск с новыми параметрами\n",
    "print(\"\\nЗапуск с обновленной конфигурацией:\")\n",
    "results, rouge_scores = run_complete_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Быстрый запуск без сохранения результатов\n",
    "update_config(\n",
    "    max_samples=100,\n",
    "    num_test_samples=50,\n",
    "    test_size= 0.5\n",
    "    save_results=False\n",
    ")\n",
    "\n",
    "print(\"\\nБыстрое тестирование:\")\n",
    "results, rouge_scores = run_complete_testing()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
